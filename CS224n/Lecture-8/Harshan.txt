Statistical Machine Translation (SMT):-
Finding best sentence y in a language given sentence x in another language.
=> argmax(P(y|x)) = argmax(P(x|y)P(y)), because P(x)=1.
We need large corpus of parallel texts.
Learning Language Alignment is hard.
Alignment is relationship between corresponding words in two languages.
It is very complex and require lot of human expertise.

Neural Machine Translation (NMT):-
* Seq2seq model
It has an encoder RNN and a decoder RNN.
The sentence is feeded into the encoder RNN and it generates an encoded hidden state at the <END> token.
This encoded hidden state is used as the initial state of the decoder RNN.
The output of each timestep in decoder RNN is used as input to the next timestep, until <END> token is generated.
The outputs of the decoder RNN is the probability distribution over the vocabulary.

Seq2seq is a conditional language model.
It is language model because it generates the probability of next word of target sentence y, but it is also conditioned on x.
=> P(y|x) = P(y1|x)P(y2|y1,x)P(y3|y2,y1,x)...P(yn|y_(n-1),...,y1,x)

Since we are only using the argmax in the output, it is a greedy approach.
However, exaustive search has exponential time complexity.
So, Beam Search is used, where the top-most k possible paths are computed.
Scoring method is to take the sum of log probabilities of each word prediction given the previous words.
The sum of log probabilities of the completed hypotheses is finally normalized by the length of the sentence, to prevent bias towards shorter sentences.
Higher scores => better.

BiLingual Evaluation Understudy (BLEU) is the evaluation metric for Machine Translation (MT).
BLEU compares machine-generated translation with the human-written translation and computes a similarity score based on:
* n-gram precision (usually 1,2,3,4-grams) (basically checks if a n-gram from machine appears atleast once in human translation)
* Brevity penalty (prevents too short sentences, which might contain only keywords and still score well in n-gram precision)
However, BLEU is imperfect because a sentence can have more than 1 translation. But if it doesn't have n-gram overlap, then it scores low.

Attention:-
The hidden state of 'a timestep' in decoder RNN is dotted with the encoder hidden states and softmax is applied over the dot-products.
We compute the attention output which weighted mean of the encoder hidden states with the weights being the attention distribution.
This attention score is concatenated with that timestep's decoder hidden state and is used to compute next word.
Sometimes, attention output from previous timestep is concatenated with the usual decoder output from previous timestep.
Sometimes, this concatenated value is then used as input to next decoder timestep.

Attention automatically learns a soft 'alignment' without explicitly being trained for it (unsupervised learning).
Attention solves bottleneck problem, helps with vanishing gradient problem and provides some interpretability.

Attention scores variants:-
Let query be s and hidden states be [h1, h2, ..., h_n]. s' means 's transposed'.
* Basic dot-product attention: (seen above)
  => e_i = s' . h_i
* Multiplicative attention:
  => e_i = s' x W x h_i
  W is learnable parameter.
* Additive attention:
  => e_i = v' x tanh(W1 x h_i + W2 x s), where v is a vector of length d
  W1, W2 and v are learnable parameters.
  The dimension size (length) of v, which is d is a hyperparameter.
